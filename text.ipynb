{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnYr/J+pnZpjMgXU9EBmjK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoon777/cv_project/blob/main/text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0Hsax8eeq9g",
        "outputId": "03c3cb02-f628-45b1-c4f6-a6036a9a185f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 0s (739 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfo9fVDNbaSu",
        "outputId": "9e103192-2a4f-4843-b3a9-ad1e541461e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.83)] [Connected to cloud.r-project.org (3.171.85.\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [2 InRelease 47.5 kB/128 kB 37%] [Waiting for headers] [Connected to cloud.r-project.org (3.171.8\r                                                                                                    \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 79.3 kB/128 kB 62%] [Waiting for headers] [3 InRelease 3,626 B/3,626 B 100%] [Connec\r0% [2 InRelease 82.2 kB/128 kB 64%] [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.1\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connecte\r                                                                                                    \rGet:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.8 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,450 kB]\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,074 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,286 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,654 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,601 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,424 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,377 kB]\n",
            "Fetched 22.3 MB in 4s (5,917 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (8,091 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123652 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdf2image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikiHHsWOdd-G",
        "outputId": "ad40588e-e8de-44db-b863-db52ae880edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.4.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfplumber pymupdf opencv-python pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqDxmIuYnZ6y",
        "outputId": "4e9f4362-2c5f-4f74-8653-8bf2715c4c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.12-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDF-1.24.12-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytesseract, pypdfium2, pymupdf, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pymupdf-1.24.12 pypdfium2-4.30.0 pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_4bHQvInONt"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "import cv2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 1. 텍스트 추출\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        full_text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                full_text += text + \"\\n\"\n",
        "    return full_text\n",
        "\n",
        "# 2. 표 추출 (텍스트 기반)\n",
        "def extract_tables_from_pdf(pdf_path):\n",
        "    tables = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            page_tables = page.extract_tables()\n",
        "            for table in page_tables:\n",
        "                tables.append(table)\n",
        "    return tables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "\n",
        "# 필요한 라이브러리들 (OCR에 필요한 라이브러리 추가)\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# 경로 설정\n",
        "pdf_path = '2024 용인 정책.pdf'\n",
        "output_folder = \"output\"\n",
        "\n",
        "# 폴더 생성\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# PDF 열기\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    num_pages = len(pdf.pages)\n",
        "\n",
        "    # 각 페이지별로 텍스트, 표, 그래프를 추출\n",
        "    for i in range(num_pages):\n",
        "        page = pdf.pages[i]\n",
        "\n",
        "        # 페이지 번호 정보\n",
        "        page_number = i + 1\n",
        "        print(f\"Processing page {page_number}...\")\n",
        "\n",
        "        # 텍스트 추출\n",
        "        print(f\"Extracting text from page {page_number}...\")\n",
        "        page_text = page.extract_text()\n",
        "\n",
        "        # 텍스트 후처리: 20자 이하인 줄은 앞 줄과 합치기\n",
        "        if page_text:\n",
        "            lines = page_text.split('\\n')\n",
        "            processed_lines = []\n",
        "            for j in range(len(lines)):\n",
        "                if j > 0 and len(lines[j]) <= 20:  # 현재 줄이 20자 이하인 경우\n",
        "                    processed_lines[-1] += \" \" + lines[j]  # 이전 줄과 합침\n",
        "                else:\n",
        "                    processed_lines.append(lines[j])  # 그렇지 않으면 그대로 추가\n",
        "            processed_text = '\\n'.join(processed_lines)\n",
        "        else:\n",
        "            processed_text = \"\"\n",
        "\n",
        "        # 텍스트 저장\n",
        "        text_output_path = os.path.join(output_folder, f\"page_{page_number}_text.txt\")\n",
        "        with open(text_output_path, 'w', encoding='utf-8') as text_file:\n",
        "            text_file.write(processed_text)\n",
        "\n",
        "        # 표 추출\n",
        "        print(f\"Extracting tables from page {page_number}...\")\n",
        "        tables = page.extract_tables()\n",
        "        if tables:\n",
        "            for idx, table in enumerate(tables):\n",
        "                df = pd.DataFrame(table)\n",
        "                table_output_path = os.path.join(output_folder, f\"page_{page_number}_table_{idx + 1}.csv\")\n",
        "                df.to_csv(table_output_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"Extraction completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1hxZTXnnYSD",
        "outputId": "bdcf77f2-15de-437f-9969-4763f396e860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page 1...\n",
            "Extracting text from page 1...\n",
            "Extracting tables from page 1...\n",
            "Processing page 2...\n",
            "Extracting text from page 2...\n",
            "Extracting tables from page 2...\n",
            "Processing page 3...\n",
            "Extracting text from page 3...\n",
            "Extracting tables from page 3...\n",
            "Processing page 4...\n",
            "Extracting text from page 4...\n",
            "Extracting tables from page 4...\n",
            "Processing page 5...\n",
            "Extracting text from page 5...\n",
            "Extracting tables from page 5...\n",
            "Processing page 6...\n",
            "Extracting text from page 6...\n",
            "Extracting tables from page 6...\n",
            "Processing page 7...\n",
            "Extracting text from page 7...\n",
            "Extracting tables from page 7...\n",
            "Processing page 8...\n",
            "Extracting text from page 8...\n",
            "Extracting tables from page 8...\n",
            "Processing page 9...\n",
            "Extracting text from page 9...\n",
            "Extracting tables from page 9...\n",
            "Processing page 10...\n",
            "Extracting text from page 10...\n",
            "Extracting tables from page 10...\n",
            "Processing page 11...\n",
            "Extracting text from page 11...\n",
            "Extracting tables from page 11...\n",
            "Processing page 12...\n",
            "Extracting text from page 12...\n",
            "Extracting tables from page 12...\n",
            "Processing page 13...\n",
            "Extracting text from page 13...\n",
            "Extracting tables from page 13...\n",
            "Processing page 14...\n",
            "Extracting text from page 14...\n",
            "Extracting tables from page 14...\n",
            "Processing page 15...\n",
            "Extracting text from page 15...\n",
            "Extracting tables from page 15...\n",
            "Processing page 16...\n",
            "Extracting text from page 16...\n",
            "Extracting tables from page 16...\n",
            "Processing page 17...\n",
            "Extracting text from page 17...\n",
            "Extracting tables from page 17...\n",
            "Processing page 18...\n",
            "Extracting text from page 18...\n",
            "Extracting tables from page 18...\n",
            "Processing page 19...\n",
            "Extracting text from page 19...\n",
            "Extracting tables from page 19...\n",
            "Processing page 20...\n",
            "Extracting text from page 20...\n",
            "Extracting tables from page 20...\n",
            "Processing page 21...\n",
            "Extracting text from page 21...\n",
            "Extracting tables from page 21...\n",
            "Processing page 22...\n",
            "Extracting text from page 22...\n",
            "Extracting tables from page 22...\n",
            "Processing page 23...\n",
            "Extracting text from page 23...\n",
            "Extracting tables from page 23...\n",
            "Processing page 24...\n",
            "Extracting text from page 24...\n",
            "Extracting tables from page 24...\n",
            "Processing page 25...\n",
            "Extracting text from page 25...\n",
            "Extracting tables from page 25...\n",
            "Processing page 26...\n",
            "Extracting text from page 26...\n",
            "Extracting tables from page 26...\n",
            "Processing page 27...\n",
            "Extracting text from page 27...\n",
            "Extracting tables from page 27...\n",
            "Processing page 28...\n",
            "Extracting text from page 28...\n",
            "Extracting tables from page 28...\n",
            "Processing page 29...\n",
            "Extracting text from page 29...\n",
            "Extracting tables from page 29...\n",
            "Processing page 30...\n",
            "Extracting text from page 30...\n",
            "Extracting tables from page 30...\n",
            "Processing page 31...\n",
            "Extracting text from page 31...\n",
            "Extracting tables from page 31...\n",
            "Processing page 32...\n",
            "Extracting text from page 32...\n",
            "Extracting tables from page 32...\n",
            "Processing page 33...\n",
            "Extracting text from page 33...\n",
            "Extracting tables from page 33...\n",
            "Processing page 34...\n",
            "Extracting text from page 34...\n",
            "Extracting tables from page 34...\n",
            "Processing page 35...\n",
            "Extracting text from page 35...\n",
            "Extracting tables from page 35...\n",
            "Processing page 36...\n",
            "Extracting text from page 36...\n",
            "Extracting tables from page 36...\n",
            "Processing page 37...\n",
            "Extracting text from page 37...\n",
            "Extracting tables from page 37...\n",
            "Processing page 38...\n",
            "Extracting text from page 38...\n",
            "Extracting tables from page 38...\n",
            "Processing page 39...\n",
            "Extracting text from page 39...\n",
            "Extracting tables from page 39...\n",
            "Processing page 40...\n",
            "Extracting text from page 40...\n",
            "Extracting tables from page 40...\n",
            "Processing page 41...\n",
            "Extracting text from page 41...\n",
            "Extracting tables from page 41...\n",
            "Processing page 42...\n",
            "Extracting text from page 42...\n",
            "Extracting tables from page 42...\n",
            "Extraction completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 경로 설정\n",
        "output_folder = \"output\"\n",
        "final_output_folder = \"final_output\"\n",
        "\n",
        "# 폴더 생성\n",
        "if not os.path.exists(final_output_folder):\n",
        "    os.makedirs(final_output_folder)\n",
        "\n",
        "# 출력 폴더 내의 파일 목록 가져오기\n",
        "files = os.listdir(output_folder)\n",
        "\n",
        "# 페이지 번호별로 텍스트 및 표 파일 결합\n",
        "pages = set(f.split('_')[1] for f in files if f.startswith('page_'))\n",
        "\n",
        "for page_number in pages:\n",
        "    # 텍스트 파일 읽기\n",
        "    text_file_path = os.path.join(output_folder, f\"page_{page_number}_text.txt\")\n",
        "    if os.path.exists(text_file_path):\n",
        "        with open(text_file_path, 'r', encoding='utf-8') as text_file:\n",
        "            lines = text_file.readlines()\n",
        "            # 텍스트 데이터를 DataFrame으로 변환 (한 줄이 한 행으로 들어가도록)\n",
        "            text_df = pd.DataFrame({'Content': [line.strip() for line in lines]})\n",
        "    else:\n",
        "        text_df = pd.DataFrame({'Content': []})\n",
        "\n",
        "    # 해당 페이지의 모든 표 파일 읽기\n",
        "    table_dfs = []\n",
        "    idx = 1\n",
        "    while True:\n",
        "        table_file_path = os.path.join(output_folder, f\"page_{page_number}_table_{idx}.csv\")\n",
        "        if os.path.exists(table_file_path):\n",
        "            table_df = pd.read_csv(table_file_path, encoding='utf-8-sig')\n",
        "            table_dfs.append(table_df)\n",
        "            idx += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # 텍스트와 표를 하나의 DataFrame으로 결합 (텍스트 뒤에 표를 바로 이어서 결합)\n",
        "    combined_dfs = [text_df]\n",
        "    for table_idx, table_df in enumerate(table_dfs, start=1):\n",
        "        # 구분자 추가\n",
        "        separator_df = pd.DataFrame({\"Content\": [f\"<{page_number} 페이지의 표 {table_idx} 내용>\"]})\n",
        "        combined_dfs.append(separator_df)\n",
        "        combined_dfs.append(table_df)\n",
        "\n",
        "    combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
        "\n",
        "    # 페이지별 결합된 CSV 저장\n",
        "    combined_output_path = os.path.join(final_output_folder, f\"page_{page_number}_combined.csv\")\n",
        "    combined_df.to_csv(combined_output_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"Merging completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MbSPZ8Ab5rF",
        "outputId": "7cc37cf7-0ab7-4112-943f-6955fb269f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지별 결합된 CSV 파일 합치기\n",
        "print(\"Combining all page CSV files into one...\")\n",
        "all_combined_df = pd.DataFrame()\n",
        "for page_number in pages:\n",
        "    file_path = os.path.join(final_output_folder, f\"page_{page_number}_combined.csv\")\n",
        "    if os.path.exists(file_path):\n",
        "        page_df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
        "        all_combined_df = pd.concat([all_combined_df, page_df], ignore_index=True)\n",
        "\n",
        "# 왼쪽에 빈 셀이 있으면 오른쪽 셀의 내용을 왼쪽으로 이동\n",
        "all_combined_df['Content'] = all_combined_df['Content'].fillna(method='ffill')\n",
        "\n",
        "# 최종 결합된 CSV 저장\n",
        "all_combined_df.to_csv('final_combined.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"All pages combined and saved to final_output/final_combined.csv.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PtnpqMogUNi",
        "outputId": "0c9b268c-486b-4037-c52f-8ab3e1479699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combining all page CSV files into one...\n",
            "All pages combined and saved to final_output/final_combined.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-f84827f5a504>:11: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  all_combined_df['Content'] = all_combined_df['Content'].fillna(method='ffill')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 최종 결합된 CSV 파일 가져오기\n",
        "final_combined_csv_path = 'final_combined.csv'\n",
        "all_combined_df = pd.read_csv(final_combined_csv_path, encoding='utf-8-sig')\n",
        "\n",
        "# 왼쪽에 빈 셀이 있으면 오른쪽 셀의 내용을 왼쪽으로 이동\n",
        "all_combined_df = all_combined_df.apply(lambda row: row.bfill(), axis=1).iloc[:, 0].to_frame(name='Content')\n",
        "\n",
        "# 최종 정리된 CSV 저장\n",
        "final_processed_csv_path = 'final_combined_processed.csv'\n",
        "all_combined_df.to_csv(final_processed_csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"Processed CSV saved to {final_processed_csv_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50JfxeRTjHt1",
        "outputId": "e9ebcb03-eb3c-431a-95d3-5ff25327da27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed CSV saved to final_combined_processed.csv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-1dba56e8c428>:8: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  all_combined_df = all_combined_df.apply(lambda row: row.bfill(), axis=1).iloc[:, 0].to_frame(name='Content')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import fitz  # PyMuPDF\n",
        "import cv2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 경로 설정\n",
        "output_folder = \"output\"\n",
        "final_output_folder = \"final_output\"\n",
        "\n",
        "# 폴더 생성\n",
        "if not os.path.exists(final_output_folder):\n",
        "    os.makedirs(final_output_folder)\n",
        "\n",
        "# 출력 폴더 내의 파일 목록 가져오기\n",
        "files = os.listdir(output_folder)\n",
        "\n",
        "# 페이지 번호별로 텍스트 및 표 파일 결합\n",
        "pages = set(f.split('_')[1] for f in files if f.startswith('page_'))\n",
        "\n",
        "for page_number in pages:\n",
        "    # 텍스트 파일 읽기\n",
        "    text_file_path = os.path.join(output_folder, f\"page_{page_number}_text.txt\")\n",
        "    if os.path.exists(text_file_path):\n",
        "        with open(text_file_path, 'r', encoding='utf-8') as text_file:\n",
        "            lines = text_file.readlines()\n",
        "            # 텍스트 데이터를 DataFrame으로 변환 (한 줄이 한 행으로 들어가도록)\n",
        "            text_df = pd.DataFrame({'Content': [line.strip() for line in lines]})\n",
        "    else:\n",
        "        text_df = pd.DataFrame({'Content': []})\n",
        "\n",
        "    # 해당 페이지의 모든 표 파일 읽기\n",
        "    table_dfs = []\n",
        "    idx = 1\n",
        "    while True:\n",
        "        table_file_path = os.path.join(output_folder, f\"page_{page_number}_table_{idx}.csv\")\n",
        "        if os.path.exists(table_file_path):\n",
        "            table_df = pd.read_csv(table_file_path, encoding='utf-8-sig')\n",
        "            table_dfs.append(table_df)\n",
        "            idx += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # 텍스트와 표를 하나의 DataFrame으로 결합 (텍스트 뒤에 표를 바로 이어서 결합)\n",
        "    combined_dfs = [text_df]\n",
        "    for table_idx, table_df in enumerate(table_dfs, start=1):\n",
        "        # 구분자 추가\n",
        "        separator_df = pd.DataFrame({\"Content\": [f\"<{page_number} 페이지의 표 {table_idx} 내용>\"]})\n",
        "        combined_dfs.append(separator_df)\n",
        "        combined_dfs.append(table_df)\n",
        "\n",
        "    combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
        "\n",
        "    # 페이지별 결합된 CSV 저장\n",
        "    combined_output_path = os.path.join(final_output_folder, f\"page_{page_number}_combined.csv\")\n",
        "    combined_df.to_csv(combined_output_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"Merging completed.\")\n",
        "\n",
        "# 페이지별 결합된 CSV 파일 합치기\n",
        "print(\"Combining all page CSV files into one...\")\n",
        "all_combined_df = pd.DataFrame()\n",
        "for file in sorted(os.listdir(final_output_folder)):\n",
        "    if file.endswith(\"_combined.csv\"):\n",
        "        file_path = os.path.join(final_output_folder, file)\n",
        "        page_df = pd.read_csv(file_path, encoding='utf-8-sig')\n",
        "        all_combined_df = pd.concat([all_combined_df, page_df], ignore_index=True)\n",
        "\n",
        "# 왼쪽에 빈 셀이 있으면 오른쪽 셀의 내용을 왼쪽으로 이동\n",
        "all_combined_df['Content'] = all_combined_df['Content'].fillna('').apply(lambda row: ' '.join([str(cell) for cell in row if str(cell).strip() != '']))\n",
        "\n",
        "# 최종 결합된 CSV 저장\n",
        "all_combined_df.to_csv('final_combined.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"All pages combined and saved to final_output/final_combined.csv.\")"
      ],
      "metadata": {
        "id": "pX2SKmDIt10Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "ac0f1fda-86ba-4fe6-9ebb-10cae4d3e5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'output_2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ab3a57de6e3e>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 출력 폴더 내의 파일 목록 가져오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 페이지 번호별로 텍스트 및 표 파일 결합\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_2'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6uHIz4mEqppx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}